# Inference Configuration for Anime Image Tagger
# This configuration controls model inference behavior and output formatting

# Model Settings
model_path: "./checkpoints/best_model.pt"  # Path to trained model checkpoint
vocab_dir: "./vocabulary"  # Directory containing vocabulary files
device: "cuda"  # Device to use: cuda, cpu, or mps (for Apple Silicon)
use_fp16: true  # Use half precision for faster inference (requires CUDA)
compile_model: false  # Whether to compile model with torch.compile (experimental)

# Image Preprocessing
image_size: 640  # Input image size (must match training)
normalize_mean: [0.485, 0.456, 0.406]  # ImageNet normalization mean
normalize_std: [0.229, 0.224, 0.225]  # ImageNet normalization std
pad_color: [114, 114, 114]  # Gray padding color for letterboxing

# Prediction Settings
prediction_threshold: 0.5  # Base threshold for tag predictions
adaptive_threshold: true  # Dynamically adjust threshold based on predictions
min_predictions: 5  # Minimum number of tags to predict per image
max_predictions: 50  # Maximum number of tags to predict per image
top_k: null  # If set, only return top-k predictions (null = no limit)

# Tag Filtering
filter_nsfw: false  # Whether to filter NSFW tags from output
nsfw_tags:  # List of NSFW tags to filter
  - explicit
  - questionable
  - sensitive
  - nude
  - sexual_content

blacklist_tags: []  # Tags to always exclude from predictions
# Example blacklist:
# blacklist_tags:
#   - watermark
#   - text
#   - signature
#   - username

whitelist_tags: null  # If set, only these tags can be predicted (null = all tags allowed)
# Example whitelist for specific use case:
# whitelist_tags:
#   - 1girl
#   - 1boy
#   - solo
#   - smile
#   - outdoor

# Post-processing
apply_implications: true  # Apply tag implications (e.g., cat_ears -> animal_ears)
resolve_aliases: true  # Resolve tag aliases to canonical forms
group_by_category: false  # Group output tags by category (character, artist, etc.)
remove_underscores: true  # Replace underscores with spaces in tag names

# Performance Settings
batch_size: 32  # Batch size for processing multiple images
num_workers: 4  # Number of data loading workers
use_tensorrt: false  # Use TensorRT optimization (requires TensorRT installation)
optimize_for_speed: false  # Apply model optimizations (may affect accuracy slightly)
batch_timeout_ms: 100  # Timeout for batch accumulation in streaming mode
max_batch_size: 32  # Maximum batch size for dynamic batching

# Output Format
output_format: "json"  # Output format: json, text, csv, xml, or yaml
include_scores: true  # Include confidence scores with predictions
score_decimal_places: 3  # Number of decimal places for scores
sort_by_score: true  # Sort tags by confidence score (highest first)

# API Settings (if running as service)
enable_api: false  # Enable REST API server
api_host: "0.0.0.0"  # API server host
api_port: 8000  # API server port
api_workers: 4  # Number of API worker processes
api_max_image_size: 10485760  # Maximum image size in bytes (10MB)
api_rate_limit: 100  # Rate limit: requests per minute
api_cors_origins:  # CORS allowed origins
  - "*"  # Allow all origins (configure properly for production)
  # - "http://localhost:3000"
  # - "https://yourdomain.com"

# Caching
enable_cache: true  # Enable prediction caching
cache_ttl_seconds: 3600  # Cache time-to-live in seconds (1 hour)
max_cache_size: 1000  # Maximum number of cached predictions

# Advanced Settings
# These settings are for fine-tuning inference behavior

# Tag Frequency Weighting
# Boost or reduce predictions based on tag frequency in training data
use_frequency_weighting: false
frequency_weight_power: 0.5  # Power to apply to frequency weights (0-1)

# Multi-Scale Inference (experimental)
# Process image at multiple scales and average predictions
multi_scale_inference: false
inference_scales: [640, 768, 896]  # Scales to use for multi-scale

# Test Time Augmentation (TTA)
# Apply augmentations during inference and average predictions
use_tta: false
tta_transforms:
  - horizontal_flip
  # - vertical_flip
  # - rotate_90
  # - rotate_270

# Region-based Inference
# Process specific regions of interest in addition to full image
enable_region_inference: false
region_crop_size: 384  # Size of region crops
region_overlap: 0.25  # Overlap between regions
combine_region_predictions: "max"  # How to combine: max, mean, or weighted

# Confidence Calibration
# Adjust prediction confidences based on calibration data
use_calibration: false
calibration_file: null  # Path to calibration parameters
temperature_scaling: 1.0  # Temperature for probability calibration

# Logging and Debugging
log_predictions: false  # Log all predictions to file
log_file: "./inference_log.json"  # Path to prediction log file
profile_inference: false  # Profile inference time per component
save_attention_maps: false  # Save attention visualizations (slow)
debug_mode: false  # Enable detailed debug output

# Model Ensemble (if using multiple models)
ensemble_models: []  # List of additional model paths for ensemble
# Example:
# ensemble_models:
#   - "./checkpoints/model_1.pt"
#   - "./checkpoints/model_2.pt"
ensemble_strategy: "mean"  # How to combine: mean, max, weighted, or vote
ensemble_weights: null  # Weights for each model (null = equal weights)

# Hardware Optimization
cuda_device_id: 0  # Specific CUDA device to use (for multi-GPU systems)
use_cuda_graphs: false  # Use CUDA graphs for optimization (experimental)
pin_memory: true  # Pin memory for faster GPU transfer
non_blocking_transfer: true  # Use non-blocking CUDA transfers

# Specialized Inference Modes
inference_mode: "standard"  # Options: standard, streaming, interactive
# streaming: Process images as they arrive
# interactive: Optimized for single-image, low-latency

# Quality Control
min_confidence_for_output: 0.1  # Minimum confidence to include tag in output
confidence_boost_popular_tags: false  # Boost confidence for frequently used tags
suppress_conflicting_tags: true  # Remove logically conflicting tags
conflict_resolution_file: null  # Path to tag conflict rules

# Monitoring and Metrics
enable_metrics: false  # Track inference metrics
metrics_port: 9090  # Port for metrics endpoint (Prometheus format)
track_gpu_usage: false  # Monitor GPU memory and utilization
alert_on_error: false  # Send alerts on inference errors
alert_webhook: null  # Webhook URL for error alerts