# Inference Configuration (migrated to nested schema)
# Keep training/inference normalization identical; use the nested keys this repo reads in code.

# Model Settings that remain top-level (referenced in various scripts)
model_path: "./checkpoints/best_model.pt"

preprocessing:
  image_size: 640
  normalize_mean: [0.5, 0.5, 0.5]
  normalize_std: [0.5, 0.5, 0.5]

runtime:
  device: "cuda"
  use_fp16: true
  tta_flip: false

postprocessing:
  threshold: 0.5
  top_k: 10
  thresholds_path: null   # If you calibrate, point at ./artifacts/thresholds.json

io:
  output_format: "json"
  save_visualizations: false
  visualization_dir: "./visualizations"

input:
  image_extensions: [".jpg", ".jpeg", ".png", ".webp"]

onnx_runtime:
  providers: ["CUDAExecutionProvider", "CPUExecutionProvider"]
