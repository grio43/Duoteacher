# DEPRECATED (migrating to unified config):
# Training knobs now live under training.* in unified_config.yaml.
# Prefer editing: configs/unified_config.yaml.
#
# NOTE: `focal_alpha_pos/neg` are deprecated in favor of unified `focal_alpha`
# (see MIGRATION.md).

# Training Configuration for Anime Image Tagger
# This configuration is for training a 1B parameter model with dual teacher distillation

# ──────────────────────────────────────────────────────────────────────────────
# QUICK START (safe knobs)
# • num_epochs: Total passes over data. Increase for better convergence; watch val loss.
# • learning_rate: Start at 1e-4 for AdamW; lower if you see divergence.
# • gradient_accumulation_steps: Multiplies effective batch size without more GPU RAM.
# • scheduler: Use `cosine_restarts` for long runs; `cosine` is simpler.
# • use_amp/amp_dtype: Mixed precision (`bfloat16` on Ampere+ is stable) → faster/less RAM.
# • save_steps/eval_steps/logging_steps: I/O cadence; larger values = fewer disk writes.
# • distributed/world_size/ddp_backend: Multi‑GPU; keep `nccl` on Linux with CUDA.
# • deterministic vs benchmark: determinism disables autotune; enable only when comparing runs.
# ──────────────────────────────────────────────────────────────────────────────

# Basic Training Settings
num_epochs: 100
learning_rate: 1.0e-4  # Base learning rate for AdamW
weight_decay: 0.01
gradient_accumulation_steps: 4  # Effective batch size = batch_size * grad_accum * world_size

# Optimizer Configuration
optimizer: adamw  # Options: adam, adamw, sgd, rmsprop, adagrad
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-8

# Learning Rate Scheduler
scheduler: cosine_restarts  # Options: cosine, cosine_restarts, step, multistep, plateau, exponential
warmup_steps: 10000         # Steps to ramp LR from 0→base; prevents early instability
# warmup_ratio: 0.0  # Deprecated - use warmup_steps directly
num_cycles: 1.0             # For cosine_restarts only; 1.0 = one full cycle
lr_end: 1.0e-6              # Floor LR (useful to keep tiny updates late in training)

# Mixed Precision Training
use_amp: true              # Enable AMP to speed up and reduce memory footprint
amp_opt_level: O1          # Legacy naming; keep O1 unless using Apex explicitly
amp_dtype: bfloat16        # Prefer bfloat16 on Ampere+ GPUs; use float16 otherwise

# Gradient Clipping
max_grad_norm: 1.0  # Maximum gradient norm for clipping

# Checkpointing Strategy
save_steps: 10000        # How often to write checkpoints
save_total_limit: 3      # Max checkpoints to retain → controls disk usage
save_best_only: false    # Set true to keep only best per val metric
eval_steps: 1000         # Validation cadence (tradeoff speed vs signal)
logging_steps: 100       # Metric/console logging cadence

# Loss Configuration - Asymmetric Focal Loss
focal_gamma_pos: 0.0  # Gamma for positive samples (0 = no focal weighting)
focal_gamma_neg: 4.0  # Gamma for negative samples (higher = focus on hard negatives)
focal_alpha_pos: 1.0  # Weight for positive samples
focal_alpha_neg: 1.0  # Weight for negative samples
label_smoothing: 0.1  # Label smoothing factor
use_class_weights: true  # Use frequency-based class weights

# Knowledge Distillation from Dual Teachers
use_distillation: false             # Enable to blend teacher logits with labels
distillation_alpha: 0.7             # 0=only labels, 1=only teachers
distillation_temperature: 3.0       # Higher = softer probability targets
anime_teacher_weight: 0.7           # Blend across teachers
clip_teacher_weight: 0.3

# Curriculum Learning Strategy
use_curriculum: true                    # Train easy→hard to stabilize early epochs
start_region_training_epoch: 20         # When to begin region emphasis
region_training_interval: 5             # Update cadence for region schedule
curriculum_difficulty_schedule: linear  # linear|exponential|step

# Hardware Configuration
device: cuda                 # Primary device: cuda|cpu|mps
distributed: false           # Set true for multi‑GPU runs (DDP)
local_rank: -1               # Populated by launcher; leave -1 for single‑GPU
world_size: 1                # Number of GPUs to use
ddp_backend: nccl            # DDP backend (use nccl on Linux/CUDA)

# Experiment Tracking
use_tensorboard: true        # Local summaries written under ./runs
use_wandb: false             # Remote experiment tracking
wandb_project: anime-tagger
wandb_run_name: null         # Auto‑named if null
wandb_entity: null           # Your W&B username/team

# Training Stability
seed: 42                      # Set for reproducibility across runs
deterministic: false          # True disables certain autotuning for repeatability
benchmark: true               # True enables cuDNN autotune (set false if deterministic)

# Early Stopping
early_stopping_patience: 10        # No improvement for N epochs → stop
early_stopping_threshold: 0.0001   # Minimal delta that counts as improvement

# Additional Settings for Production Training
# These can be uncommented and adjusted as needed

# Data Augmentation (handled in DataConfig, but can influence training)
# augmentation_probability: 0.5
# mixup_alpha: 0.2
# cutmix_alpha: 0.0

# Advanced Optimizer Settings
# gradient_centralization: false
# lookahead_k: 0
# lookahead_alpha: 0.5

# Learning Rate Schedule Fine-tuning
# polynomial_power: 1.0
# cycle_momentum: true
# base_momentum: 0.85
# max_momentum: 0.95

# Memory Optimization
# gradient_checkpointing: true  # Trade compute for memory
# find_unused_parameters: false  # For DDP optimization

# Validation Settings
# val_check_interval: 1.0  # Check validation every epoch
# limit_val_batches: 1.0  # Use full validation set
# num_sanity_val_steps: 2  # Sanity check steps before training

# Progressive Training (for scaling from 1B to 3B)
# progressive_unfreezing: false
# unfreeze_schedule: [20, 40, 60, 80]  # Epochs to unfreeze layers
# layer_wise_lr_decay: 0.95  # Learning rate decay per layer

# Loss Weights for Multi-Task Learning
# main_task_weight: 0.8
# auxiliary_task_weight: 0.2

# Logging Verbosity
# log_level: INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
# log_gradients: false
# log_weights: false
# log_activations: false

# Performance Monitoring
# profile_training: false
# profile_memory: false
# detect_anomaly: false  # Enable for debugging NaN issues

# Dataset-Specific Settings (override in full config)
# num_workers: 8
# prefetch_factor: 2
# persistent_workers: true
# pin_memory: true
