
{
"summary": "Focused review for durability and bugs with current defaults (no tag filter changes). Priority is avoiding any loss of files or per-image JSON metadata while pulling from RAID-5 to RAID-0.",
"priority_order": [
"mmapping-dup-and-leak",
"tar-batch-clear-race",
"durability-on-signal",
"mmap-fallback-instability",
"basename-collision-skips",
"ext-detection-miss",
"unbounded-not-found-list",
"polars-api-warnings",
"write-buffer-cap-mismatch",
"eager-shard-dirs"
],
"issues": [
{
"id": "mmapping-dup-and-leak",
"severity": "blocker",
"area": "I/O caching & mmap",
"evidence": [
"Each large tar logs twice in quick succession: ‚Äúüìç Memory-mapped <tar> ‚Ä¶‚Äù followed by ‚Äú‚úÖ Using mmap for large tar: <tar>‚Äù, and this pair itself appears twice per tar (e.g., 0001.tar, 0002.tar, etc.).",
"Default config shows use_memory_mapping disabled, yet large tar policy still memory-maps files, so mmaps are created even when not actually used."
],
"impact": "Duplicate memory maps per tar can leak resources and fragment address space. It also explains intermittent mmap failures later. If the mmapped handle isn‚Äôt used, you burn kernel resources for no benefit.",
"recommendation": "Create mmaps only when they will be used (gate mapping under the same switch used for consuming mmaps). Coordinate mapping with a single ‚Äòloading‚Äô guard so only one thread maps a given tar; all others should wait for the handle. Ensure the loser thread does not keep a stray mapped handle."
},
{
"id": "tar-batch-clear-race",
"severity": "blocker",
"area": "concurrency",
"evidence": [
"A tar‚Äôs batch is read under a lock, then the list is cleared after the lock is released.",
"While between those two moments, producers can append new items that get discarded when the batch is cleared."
],
"impact": "Images may be silently dropped from extraction, causing missing files and mismatched metadata.",
"recommendation": "Clear or swap the batch under the same lock used to read it. Use a take-and-replace pattern so no appends can be lost."
},
{
"id": "durability-on-signal",
"severity": "major",
"area": "shutdown behavior",
"evidence": [
"On first Ctrl-C the run attempts a graceful stop; on the second Ctrl-C it force-exits the process.",
"Per-image JSON writes are buffered on a background thread; progress JSON also uses periodic saves. With fsync disabled by default, a forced exit can drop recent files/metadata."
],
"impact": "Recent images or JSON may vanish if the system receives a second interrupt (or crashes) before the buffers flush.",
"recommendation": "Before a hard exit, block briefly to flush pending image renames and JSON writes (including a final fsync when enabled). Consider writing a small ‚Äòlast batch‚Äô manifest before extraction and clearing it after the metadata is durably written."
},
{
"id": "mmap-fallback-instability",
"severity": "major",
"area": "I/O on network/RAID source",
"evidence": [
"Intermittent messages: ‚Äú‚ö†Ô∏è mmap requested but failed for 0017.tar/0027.tar/0030.tar/0031.tar/0036.tar/0045.tar/0048.tar. Falling back to direct reads.‚Äù"
],
"impact": "Mixed access modes (some mmapped, some direct) can cause uneven throughput and extra retries. On some network or NAS mounts, mmap is unreliable.",
"recommendation": "Use a single, consistent policy. If the source filesystem intermittently rejects mmaps, prefer a ‚Äòdirect read‚Äô policy for large tars on that mount, or enable a detected fallback mode that disables further mmaps for the session after N failures."
},
{
"id": "basename-collision-skips",
"severity": "major",
"area": "member lookup inside tars",
"evidence": [
"Member indices are keyed by basename only; when multiple entries share the same basename, the default policy is to skip extraction.",
"This can silently reduce recall when archives contain nested directories with duplicate basenames."
],
"impact": "Valid images can be skipped, creating gaps in the output and metadata.",
"recommendation": "Prefer full-path matches when available; otherwise implement a consistent disambiguation strategy that doesn‚Äôt silently drop candidates (e.g., shortest path or top-level preference with logging that surfaces the decision)."
},
{
"id": "ext-detection-miss",
"severity": "major",
"area": "ID‚Üífilename resolution",
"evidence": [
"When the sidecar has no path, the extension is inferred from the URL or defaulted (.jpg). If the actual member is a different extension, the lookup misses and the image is treated as not found."
],
"impact": "Legitimate files are counted as not found and never extracted, diverging files vs. metadata.",
"recommendation": "At fallback time, probe a small set of common extensions for the same ID before declaring not found, and cache the discovered path for subsequent rows."
},
{
"id": "unbounded-not-found-list",
"severity": "minor",
"area": "memory usage",
"evidence": [
"A list of individual not-found IDs is appended for the entire run."
],
"impact": "On large runs this list can grow very large and add GC pressure.",
"recommendation": "Bound this list or sample it; also persist periodic snapshots so you don‚Äôt lose the audit trail on crash."
},
{
"id": "polars-api-warnings",
"severity": "minor",
"area": "metadata streaming",
"evidence": [
"PerformanceWarning: accessing LazyFrame columns triggers schema resolution.",
"DeprecationWarning: ‚Äòstreaming‚Äô parameter deprecated; use the new engine parameter."
],
"impact": "No data loss, but possible slowdowns or future breakage.",
"recommendation": "Fetch column names via the fast schema call and switch to the current streaming engine parameter to avoid deprecation issues."
},
{
"id": "write-buffer-cap-mismatch",
"severity": "minor",
"area": "I/O tuning visibility",
"evidence": [
"Configured write buffer is 256MB, but the implementation caps it at 64MB."
],
"impact": "Operator expects larger buffers that never actually apply; can mislead performance tuning.",
"recommendation": "Document the cap in logs or align the cap with the configuration value to avoid confusion."
},
{
"id": "eager-shard-dirs",
"severity": "nit",
"area": "validation side-effects",
"evidence": [
"Validation checks call into shard path resolution that creates directories on each look-up."
],
"impact": "Extra directory churn during validation; not a correctness issue.",
"recommendation": "Gate directory creation behind writes, not existence checks, or cache shard existence checks to reduce mkdir syscalls."
}
],
"data_integrity_hardening": [
"Keep atomic temp‚Üífinal renames in the same directory (you already do this).",
"Persist a simple ‚ÄòWAL/manifest‚Äô for the current in-flight batch of IDs; clear it immediately after per-image JSON is durably written.",
"Checkpoint progress and not-found summaries on a fixed cadence and at natural boundaries (end of each tar), with optional fsync.",
"On first Ctrl-C: stop enqueuing new work, wait for pending writes to finish, flush JSON/progress, then exit. On second Ctrl-C: attempt a short grace period to flush before forcing exit.",
"After crash/restart: re-validate the last N shards and reconcile metadata (existing image files should suppress duplicate work)."
],
"operational_notes_from_your_run": {
"workers": "24 extraction, 12 I/O (healthy split).",
"memory": "Climbed from ~11GB to ~25GB early in the run; consistent with multiple large mmaps and caches.",
"throughput": "Write rate fluctuated (7‚Äì21 MB/s) with very small ‚ÄòTotal written‚Äô‚Äîexpected early if only a few images match per tar.",
"mmap_behavior": "Many large tars were memory-mapped; several failed and fell back to direct reads. This is consistent with a NAS or mount that inconsistently supports mmap."
}
}