diff --git a/src/data/HDF5_loader.py b/src/data/HDF5_loader.py
new file mode 100644
index 0000000..12eb662
--- /dev/null
+++ b/src/data/HDF5_loader.py
@@ -0,0 +1,700 @@
+#!/usr/bin/env python3
+"""
+Data loading and augmentation utilities for the anime tagger.
+
+This module provides a simplified HDF5/JSON loader with support for
+letterbox-style resizing.  Images are resized to fit within a square
+canvas while preserving aspect ratio and padded with a neutral colour.
+Padding information is returned so downstream modules can mask out
+padded regions (e.g. during vision transformer patchification).
+
+The default pad colour is a mid‑grey (114,114,114) as commonly used by
+YOLO models.  The pad colour and patch size are configurable via
+``SimplifiedDataConfig``.
+"""
+
+from __future__ import annotations
+from dataclasses import dataclass
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+from pathlib import Path
+import json
+import threading
+import logging
+
+import numpy as np
+import torch
+from torch.utils.data import DataLoader, Dataset, DistributedSampler, WeightedRandomSampler
+import torchvision.transforms as T
+import torchvision.transforms.functional as TF
+from PIL import Image
+
+from collections import OrderedDict
+
+from orientation_handler import OrientationHandler, OrientationMonitor  # type: ignore
+from ..utils.vocabulary import TagVocabulary
+
+logger = logging.getLogger(__name__)
+
+
+def letterbox_resize(
+    image: torch.Tensor,
+    target_size: int,
+    pad_color: Iterable[int] = (114, 114, 114),
+    patch_size: int = 16,
+) -> Tuple[torch.Tensor, Dict[str, Any]]:
+    """
+    Resize an image tensor to a square canvas while preserving aspect ratio.
+
+    The input image is first scaled by the minimal factor required to fit
+    within the ``target_size``.  Any remaining space is padded equally on
+    each side using the provided ``pad_color``.  If the resulting
+    dimensions are not divisible by ``patch_size`` then additional padding
+    is applied to the bottom and right edges so that the final width and
+    height are multiples of ``patch_size``.  This ensures that when the
+    image is partitioned into non‑overlapping patches (e.g. for a vision
+    transformer) no partial patches are dropped.
+
+    Args:
+        image: Input image tensor of shape (C, H, W) with values in [0, 1].
+        target_size: Desired square size (both height and width) of the output.
+        pad_color: RGB colour used to fill padded regions.  Values should be
+            integers in the 0‑255 range.
+        patch_size: Patch size used by the downstream model.  The output
+            dimensions will be rounded up to the next multiple of this value.
+
+    Returns:
+        A tuple containing:
+          * The padded image tensor of shape (C, H_out, W_out).
+          * A dictionary with keys ``scale`` and ``pad`` describing the
+            applied scaling factor and padding on each side (left, top,
+            right, bottom).  These values can be used to derive padding
+            masks during patchification.
+    """
+    c, h, w = image.shape
+    # Compute scaling factor to fit the longer side into target_size.
+    r = min(target_size / float(h), target_size / float(w))
+    # Avoid degenerate new sizes.
+    new_h = int(round(h * r))
+    new_w = int(round(w * r))
+    # Resize using bilinear interpolation on tensors.
+    resized = TF.resize(image, [new_h, new_w], interpolation=T.InterpolationMode.BILINEAR)
+    # Compute padding needed to reach target_size.
+    pad_h = target_size - new_h
+    pad_w = target_size - new_w
+    pad_top = pad_h // 2
+    pad_bottom = pad_h - pad_top
+    pad_left = pad_w // 2
+    pad_right = pad_w - pad_left
+    # After initial letterbox, ensure divisibility by patch_size.
+    final_h = target_size
+    final_w = target_size
+    extra_pad_bottom = 0
+    extra_pad_right = 0
+    if patch_size is not None and patch_size > 1:
+        if final_h % patch_size != 0:
+            extra_pad_bottom = patch_size - (final_h % patch_size)
+        if final_w % patch_size != 0:
+            extra_pad_right = patch_size - (final_w % patch_size)
+    final_h += extra_pad_bottom
+    final_w += extra_pad_right
+    # Create canvas and fill with pad colour.
+    canvas = torch.zeros((c, final_h, final_w), dtype=resized.dtype)
+    # Normalise pad colour to [0,1] range.
+    pad_vals = torch.tensor(pad_color, dtype=resized.dtype) / 255.0
+    for ch in range(c):
+        canvas[ch, :, :] = pad_vals[ch]
+    # Paste resized image into the centre region.
+    start_y = pad_top
+    end_y = pad_top + new_h
+    start_x = pad_left
+    end_x = pad_left + new_w
+    canvas[:, start_y:end_y, start_x:end_x] = resized
+    info = {
+        "scale": r,
+        "pad": (pad_left, pad_top, pad_right + extra_pad_right, pad_bottom + extra_pad_bottom),
+        "out_size": (final_h, final_w),
+        "in_size": (h, w),
+    }
+    return canvas, info
+
+
+@dataclass
+class SimplifiedDataConfig:
+    """
+    Configuration parameters controlling data loading and augmentation.
+
+    These settings are tailored for high‑resolution anime artwork.  The
+    ``pad_color`` is used during letterbox resizing to fill any empty
+    regions and should match the neutral colour used during inference.
+    ``patch_size`` defines the patch size expected by the vision
+    transformer; ``image_size`` should be divisible by ``patch_size``.
+    """
+
+    # Required locations
+    data_dir: Path
+    json_dir: Path
+    vocab_path: Path
+
+    # Image settings
+    image_size: int = 640
+    # Normalisation parameters (defaults tuned for anime artwork rather than ImageNet)
+    normalize_mean: Tuple[float, float, float] = (0.5, 0.5, 0.5)
+    normalize_std: Tuple[float, float, float] = (0.5, 0.5, 0.5)
+    # Padding colour used for letterbox resizing (RGB in 0‑255 range).
+    pad_color: Tuple[int, int, int] = (114, 114, 114)
+    # Patch size for downstream model.  ``image_size`` should be divisible by this.
+    patch_size: int = 16
+
+    # Vocabulary settings
+    top_k_tags: int = 100_000
+    min_tag_frequency: int = 1  # include all tags that appear at least once
+
+    # Augmentation settings
+    augmentation_enabled: bool = True
+    # Reduce flip probability because many tags encode left/right semantics
+    random_flip_prob: float = 0.2
+    # Narrow crop scale range to preserve most of the subject in the frame.
+    # When (1.0, 1.0) no random cropping is performed.
+    random_crop_scale: Tuple[float, float] = (0.95, 1.0)
+    # Colour jitter parameters
+    color_jitter: bool = True
+    color_jitter_brightness: float = 0.4
+    color_jitter_contrast: float = 0.4
+    color_jitter_saturation: float = 0.4
+    color_jitter_hue: float = 0.1
+    # Optional path to orientation mapping (JSON or YAML).  If provided, the
+    # mapping is loaded on dataset initialisation.
+    orientation_map_path: Optional[Path] = None
+
+    # Sampling settings
+    frequency_weighted_sampling: bool = True
+    sample_weight_power: float = 0.5
+    orientation_oversample_factor: float = 2.0
+
+    # Multi‑GPU settings
+    distributed: bool = False
+    rank: int = 0
+    world_size: int = 1
+
+    # Cache settings
+    cache_size_gb: float = 8.0
+    cache_precision: str = 'float16'  # 'float32', 'float16' or 'uint8'
+    preload_metadata: bool = True
+
+    def __post_init__(self) -> None:
+        # Validate flip probability
+        if not 0.0 <= self.random_flip_prob <= 1.0:
+            raise ValueError(f"random_flip_prob must be in [0, 1], got {self.random_flip_prob}")
+        # Validate cache precision
+        if self.cache_precision not in {'float32', 'float16', 'uint8'}:
+            raise ValueError(
+                f"cache_precision must be one of 'float32', 'float16' or 'uint8', got {self.cache_precision}"
+            )
+        # Ensure image_size divisible by patch_size
+        if self.image_size % self.patch_size != 0:
+            logger.warning(
+                f"image_size ({self.image_size}) is not divisible by patch_size ({self.patch_size}); "
+                f"letterbox_resize will pad further to the next multiple."
+            )
+
+
+class SimplifiedDataset(Dataset):
+    """Dataset for anime image tagging with augmentation and sampling.
+
+    Each item returned is a dictionary containing an image tensor,
+    multi‑hot tag labels, a rating label and some metadata (index, path,
+    original tag list and rating string).  Letterbox resizing is applied
+    on the fly to preserve the original aspect ratio and avoid dropping
+    edge pixels when splitting into patches.  Padding information is
+    returned in the metadata for use by downstream modules.
+    """
+
+    def __init__(
+            self,
+            config: SimplifiedDataConfig,
+            json_files: List[Path],
+            split: str,
+            vocab: TagVocabulary,
+        ) -> None:
+            assert split in {'train', 'val', 'test'}, f"Unknown split '{split}'"
+            self.config = config
+            self.split = split
+            self.vocab = vocab
+            # List of annotation dictionaries loaded from JSON files
+            self.annotations: List[Dict[str, Any]] = []
+            # LRU cache for loaded images
+            self.cache: OrderedDict[str, torch.Tensor] = OrderedDict()
+            # Initialize orientation handler for flip augmentation
+            # Only needed for training split when flips are enabled
+            if split == 'train' and config.random_flip_prob > 0:
+                self.orientation_handler = OrientationHandler(
+                    mapping_file=config.orientation_map_path,
+                    random_flip_prob=config.random_flip_prob,
+                    strict_mode=False,  # Don't fail if mapping is incomplete
+                    skip_unmapped=True   # Skip flipping images with unmapped orientation tags
+                )
+
+                # Pre-compute mappings if vocabulary is available for better performance
+                if vocab and hasattr(vocab, 'tag_to_index'):
+                    all_tags = set(vocab.tag_to_index.keys())
+                    self.precomputed_mappings = self.orientation_handler.precompute_all_mappings(all_tags)
+
+                    # Validate mappings and log any issues
+                    validation_issues = self.orientation_handler.validate_dataset_tags(all_tags)
+                    if validation_issues:
+                        logger.warning(f"Orientation mapping validation issues: {validation_issues}")
+
+                        # Save validation report for review
+                        from pathlib import Path as _Path  # avoid namespace confusion in compiled docs
+                        validation_report_path = _Path("orientation_validation_report.json")
+                        with open(validation_report_path, 'w') as f:
+                            json.dump(validation_issues, f, indent=2)
+                        logger.info(f"Saved validation report to {validation_report_path}")
+
+                        # Optionally fail if strict validation is enabled
+                        if hasattr(config, 'strict_orientation_validation') and config.strict_orientation_validation:
+                            raise ValueError(f"Critical orientation mapping issues found. Check {validation_report_path}")
+                else:
+                    self.precomputed_mappings = None
+
+                # Initialize monitor for training
+                self.orientation_monitor = OrientationMonitor(threshold_unmapped=20)
+            else:
+                self.orientation_handler = None
+                self.precomputed_mappings = None
+                self.orientation_monitor = None
+            # Set up augmentation and normalisation
+            self.augmentation = self._setup_augmentation() if config.augmentation_enabled and split == 'train' else None
+            self.normalize = T.Normalize(mean=config.normalize_mean, std=config.normalize_std)
+            # Determine maximum cache size in number of images
+            bytes_per_element = {
+                'float32': 4,
+                'float16': 2,
+                'uint8': 1,
+            }[config.cache_precision]
+            # Approximate bytes per cached image by assuming they will be stored at config.image_size
+            bytes_per_image = 3 * config.image_size * config.image_size * bytes_per_element
+            self.max_cache_size = int((config.cache_size_gb * (1024 ** 3)) / bytes_per_image)
+            self.cache_precision = config.cache_precision
+            self.cache_lock = threading.Lock()  # Add thread lock for cache operations
+
+            # Initialise locks and counters for error handling.  `_error_counts` is
+            # used to track temporary I/O failures per image and must be
+            # accessed under `_error_counts_lock` to ensure thread safety.
+            self._error_counts_lock = threading.Lock()
+            self._error_counts: Dict[str, int] = {}
+
+            # Load annotation metadata from the provided JSON files.  This
+            # populates ``self.annotations`` with valid entries.
+            self._load_annotations(json_files)
+            # Compute sampling weights if frequency‑weighted sampling is
+            # enabled and this is the training split; otherwise, assign
+            # ``None`` so that standard shuffling is used.
+            if self.config.frequency_weighted_sampling and split == 'train':
+                self._calculate_sample_weights()
+            else:
+                self.sample_weights = None
+
+            logger.info(f"Dataset initialised with {len(self.annotations)} samples for split '{split}'")
+
+    def _load_annotations(self, json_files: List[Path]) -> None:
+        """Parse annotation files and populate ``self.annotations``.
+
+        Images with at least one tag are kept.  Unknown tags are retained
+        (they will map to ``<UNK>`` when encoding).  Missing or invalid
+        entries are skipped silently but logged.
+        """
+        for json_file in json_files:
+            try:
+                with open(json_file, 'r', encoding='utf-8') as f:
+                    data = json.load(f)
+                for entry in data:
+                    filename = entry.get('filename')
+                    tags_field = entry.get('tags')
+                    if not filename or not tags_field:
+                        continue
+                    tags_list: List[str]
+                    if isinstance(tags_field, str):
+                        tags_list = tags_field.split()
+                    elif isinstance(tags_field, list):
+                        tags_list = tags_field
+                    else:
+                        continue
+                    # Build annotation record
+                    record: Dict[str, Any] = {
+                        'image_path': str(self.config.data_dir / filename),
+                        'tags': tags_list,
+                        'rating': entry.get('rating', 'unknown'),
+                        'num_tags': len(tags_list),
+                    }
+                    self.annotations.append(record)
+            except Exception as e:
+                logger.warning(f"Failed to parse {json_file}: {e}")
+
+    def _calculate_sample_weights(self) -> None:
+        """Compute sampling weights for frequency‑weighted sampling."""
+        weights: List[float] = []
+        # Build a set of tags that influence orientation oversampling
+        orientation_tags = set()
+        if self.orientation_handler is not None and self.precomputed_mappings:
+            orientation_tags.update(self.orientation_handler.explicit_mappings.keys())
+            orientation_tags.update(self.orientation_handler.reverse_mappings.keys())
+        for anno in self.annotations:
+            w = 0.0
+            has_orientation_tag = False
+            for tag in anno['tags']:
+                freq = self.vocab.tag_frequencies.get(tag, 1)
+                # Inverse frequency weighting
+                w += (1.0 / max(freq, 1)) ** self.config.sample_weight_power
+                if tag in orientation_tags:
+                    has_orientation_tag = True
+            # Average over number of tags to avoid biasing multi‑tag images
+            w = w / max(1, len(anno['tags']))
+            # Apply orientation oversample factor if needed
+            if has_orientation_tag:
+                w *= self.config.orientation_oversample_factor
+            weights.append(w)
+        weights_arr = np.array(weights, dtype=np.float64)
+        # Normalise weights to sum to one
+        weights_arr = weights_arr / weights_arr.sum() if weights_arr.sum() > 0 else weights_arr
+        self.sample_weights = weights_arr
+        logger.info(
+            f"Sample weights calculated (min={weights_arr.min():.6f}, max={weights_arr.max():.6f})"
+        )
+
+    def _setup_augmentation(self) -> Optional[T.Compose]:
+        """Create an augmentation pipeline for the training split.
+
+        The pipeline excludes horizontal flips, which are handled explicitly in
+        :meth:`__getitem__` to enable orientation‑aware tag remapping.  Colour
+        jitter parameters are configurable via :class:`SimplifiedDataConfig`.
+        A random gamma transform with a wide range is appended to better
+        handle exposure variations.
+        """
+        transforms: List[Any] = []
+        # Random resized crop
+        if self.config.random_crop_scale != (1.0, 1.0):
+            transforms.append(
+                T.RandomResizedCrop(
+                    self.config.image_size,
+                    scale=self.config.random_crop_scale,
+                    ratio=(0.9, 1.1),
+                    interpolation=T.InterpolationMode.LANCZOS,
+                )
+            )
+        # Colour jitter
+        if self.config.color_jitter:
+            transforms.append(
+                T.ColorJitter(
+                    brightness=self.config.color_jitter_brightness,
+                    contrast=self.config.color_jitter_contrast,
+                    saturation=self.config.color_jitter_saturation,
+                    hue=self.config.color_jitter_hue,
+                )
+            )
+        # Random gamma correction
+        def gamma_transform(img: torch.Tensor) -> torch.Tensor:
+            gamma = float(np.random.uniform(0.7, 1.3))
+            return TF.adjust_gamma(img, gamma=gamma)
+        transforms.append(T.Lambda(gamma_transform))
+        return T.Compose(transforms) if transforms else None
+
+    def __len__(self) -> int:
+        return len(self.annotations)
+
+    def _load_image(self, image_path: str) -> torch.Tensor:
+        """Load an image from disk or cache and return it as a float tensor.
+
+        Images are loaded at their original resolution without resizing.
+        Resizing and padding are handled later via ``letterbox_resize``.
+        A copy of the image may be cached to accelerate repeated accesses.
+        The cache stores images in the precision specified by
+        ``cache_precision``; loaded images are always returned as ``float32``
+        tensors in the [0, 1] range.
+        """
+        # Check cache first
+        with self.cache_lock:
+            if image_path in self.cache:
+                cached = self.cache[image_path]
+                # Move to end to mark as recently used (LRU behavior)
+                self.cache.move_to_end(image_path)
+                # Convert cached tensor back to float32
+                if cached.dtype == torch.uint8:
+                    return cached.float() / 255.0
+                elif cached.dtype == torch.float16:
+                    return cached.float()
+                else:
+                    return cached.clone()
+        # Load from disk
+        try:
+            image = Image.open(image_path).convert('RGB')
+            tensor = TF.to_tensor(image)  # returns float32 in [0, 1]
+            # Add to cache with LRU eviction if needed
+            if self.cache_precision == 'uint8':
+                cached_tensor = (tensor * 255).to(torch.uint8)
+            elif self.cache_precision == 'float16':
+                cached_tensor = tensor.half()
+            else:
+                cached_tensor = tensor.clone()
+            with self.cache_lock:
+                if len(self.cache) >= self.max_cache_size:
+                    # Remove the least recently used item (first item in OrderedDict)
+                    self.cache.popitem(last=False)
+                self.cache[image_path] = cached_tensor
+            return tensor
+        except Exception as e:
+            logger.error(f"Error loading image {image_path}: {e}")
+            # Return black image to avoid crashing caller
+            return torch.zeros(3, self.config.image_size, self.config.image_size, dtype=torch.float32)
+
+    def __getitem__(self, idx: int) -> Dict[str, Any]:
+        """Fetch an item for training or validation.
+
+        Applies a random horizontal flip with orientation‑aware tag remapping
+        for the training split.  Letterbox resizing is performed to
+        preserve aspect ratio.  Augmentation and normalisation are then
+        applied.
+
+        Args:
+            idx: Index of item to fetch
+
+        Returns:
+            Dictionary containing image, labels, and metadata
+
+        Raises:
+            IndexError: If index is out of bounds
+            RuntimeError: If critical error occurs during processing
+        """
+        if idx < 0 or idx >= len(self.annotations):
+            raise IndexError(f"Index {idx} out of range for dataset with {len(self.annotations)} samples")
+        anno = self.annotations[idx]
+        image_path = anno['image_path']
+        # Track error count for this specific image (thread‑safe).
+        with self._error_counts_lock:
+            error_count = self._error_counts.get(image_path, 0)
+        max_retries = 3
+        try:
+            # Load image tensor (float32)
+            image = self._load_image(image_path)
+            # Copy tags so we can mutate without altering the original
+            tags = list(anno['tags'])
+            # Random horizontal flip with orientation-aware tag swapping
+            if (
+                self.orientation_handler is not None
+                and self.split == 'train'
+                and np.random.rand() < self.config.random_flip_prob
+            ):
+                swapped_tags, should_flip = self.orientation_handler.handle_complex_tags(tags)
+                if should_flip:
+                    image = TF.hflip(image)
+                    tags = swapped_tags
+                    if self.orientation_monitor:
+                        self.orientation_monitor.check_health(self.orientation_handler)
+            # Perform letterbox resize to preserve aspect ratio
+            image, lb_info = letterbox_resize(
+                image,
+                target_size=self.config.image_size,
+                pad_color=self.config.pad_color,
+                patch_size=self.config.patch_size,
+            )
+            # Apply additional augmentation (colour jitter, gamma) if enabled
+            if self.augmentation is not None:
+                image = self.augmentation(image)
+            # Normalise
+            image = self.normalize(image)
+            # Encode tags and rating
+            tag_labels = self.vocab.encode_tags(tags)
+            rating_label = self.vocab.rating_to_index.get(anno['rating'], self.vocab.rating_to_index['unknown'])
+            # Reset error count on successful load.
+            with self._error_counts_lock:
+                if image_path in self._error_counts:
+                    del self._error_counts[image_path]
+            # Package the sample as a dictionary.  Labels are nested
+            # under a single ``labels`` key to avoid duplication.  Tag
+            # labels are returned as a multi‑hot vector and rating as an
+            # integer index.  Include scaling and padding information in
+            # metadata for downstream use.
+            return {
+                'image': image,
+                'labels': {
+                    'tags': tag_labels,
+                    'rating': rating_label,
+                },
+                'metadata': {
+                    'index': idx,
+                    'path': anno['image_path'],
+                    'num_tags': anno['num_tags'],
+                    'tags': tags,
+                    'rating': anno['rating'],
+                    'scale': lb_info['scale'],
+                    'pad': lb_info['pad'],
+                },
+            }
+        except (IOError, OSError) as e:
+            # File I/O errors – may be temporary.  Increment the retry count
+            # under the lock.  If the maximum number of retries is
+            # exceeded, propagate a runtime error to abort training.
+            with self._error_counts_lock:
+                self._error_counts[image_path] = error_count + 1
+            if error_count >= max_retries:
+                logger.error(f"Failed to load {image_path} after {max_retries} attempts: {e}")
+                raise RuntimeError(f"Persistent failure loading {image_path}: {e}") from e
+            else:
+                logger.warning(f"Error loading {image_path} (attempt {error_count + 1}/{max_retries}): {e}")
+                return self._create_error_sample(idx, image_path, "io_error")
+        except Exception as e:
+            # Unexpected errors should not be silenced
+            logger.error(f"Unexpected error in __getitem__ for index {idx}, path {image_path}: {e}")
+            raise RuntimeError(f"Unexpected error processing sample {idx}") from e
+
+    def _create_error_sample(self, idx: int, image_path: str, error_type: str) -> Dict[str, Any]:
+        """Create an error sample with appropriate defaults.
+
+        This should only be used for recoverable errors like temporary I/O issues.
+        """
+        logger.debug(f"Creating error sample for {image_path}, type: {error_type}")
+        # Use letterbox padding on a zero image so downstream patchify has correct shape.
+        blank = torch.zeros(3, self.config.image_size, self.config.image_size, dtype=torch.float32)
+        padded, lb_info = letterbox_resize(
+            blank,
+            target_size=self.config.image_size,
+            pad_color=self.config.pad_color,
+            patch_size=self.config.patch_size,
+        )
+        return {
+            'image': padded,
+            'labels': {
+                'tags': torch.zeros(len(self.vocab.tag_to_index), dtype=torch.float32),
+                'rating': torch.tensor(self.vocab.rating_to_index.get('unknown', 4), dtype=torch.long),
+            },
+            'metadata': {
+                'index': idx,
+                'path': image_path,
+                'num_tags': 0,
+                'tags': [],
+                'rating': 'unknown',
+                'error_type': error_type,
+                'scale': lb_info['scale'],
+                'pad': lb_info['pad'],
+            },
+        }
+
+
+def create_dataloaders(
+    data_dir: Path,
+    json_dir: Path,
+    vocab_path: Path,
+    batch_size: int = 32,
+    num_workers: int = 8,
+    distributed: bool = False,
+    rank: int = 0,
+    world_size: int = 1,
+    frequency_sampling: bool = True,
+    val_batch_size: Optional[int] = None,
+    config_updates: Optional[Dict[str, Any]] = None,
+) -> Tuple[DataLoader, DataLoader, TagVocabulary]:
+    """Construct training and validation dataloaders along with the vocabulary.
+
+    Splits JSON annotation files into 90 % training and 10 % validation.  If a
+    vocabulary file exists it is loaded; otherwise it is built from all
+    annotations and saved.  The returned validation batch size defaults to
+    ``batch_size`` if not explicitly provided.  Both dataloaders use a
+    custom collate function defined below.
+    """
+    json_files = list(json_dir.glob("*.json"))
+    if not json_files:
+        raise ValueError(f"No JSON files found in {json_dir}")
+    # Shuffle and split files
+    json_files_sorted = sorted(json_files)
+    np.random.shuffle(json_files_sorted)
+    split_idx = int(len(json_files_sorted) * 0.9)
+    train_files = json_files_sorted[:split_idx]
+    val_files = json_files_sorted[split_idx:]
+    # Instantiate config and vocabulary
+    cfg = SimplifiedDataConfig(
+        data_dir=data_dir,
+        json_dir=json_dir,
+        vocab_path=vocab_path,
+        distributed=distributed,
+        rank=rank,
+        world_size=world_size,
+        frequency_weighted_sampling=frequency_sampling,
+    )
+    # Apply any user‑provided overrides
+    if config_updates:
+        for k, v in config_updates.items():
+            if hasattr(cfg, k):
+                setattr(cfg, k, v)
+    vocab = TagVocabulary(vocab_path, min_frequency=cfg.min_tag_frequency)
+    if not vocab_path.exists():
+        # Build vocabulary from all available annotations
+        vocab.build_from_annotations(json_files_sorted, cfg.top_k_tags)
+        vocab.save_vocabulary(vocab_path)
+    # Create datasets
+    train_dataset = SimplifiedDataset(cfg, train_files, split='train', vocab=vocab)
+    val_dataset = SimplifiedDataset(cfg, val_files, split='val', vocab=vocab)
+    # Choose sampler
+    train_sampler: Optional[Any] = None
+    if distributed:
+        train_sampler = DistributedSampler(
+            train_dataset,
+            num_replicas=world_size,
+            rank=rank,
+            shuffle=True,
+        )
+    elif frequency_sampling and train_dataset.sample_weights is not None:
+        train_sampler = WeightedRandomSampler(
+            weights=train_dataset.sample_weights,
+            num_samples=len(train_dataset),
+            replacement=True,
+        )
+    # Determine validation batch size
+    val_bs = val_batch_size or batch_size
+    # Build dataloaders
+    train_loader = DataLoader(
+        train_dataset,
+        batch_size=batch_size,
+        shuffle=(train_sampler is None),
+        sampler=train_sampler,
+        num_workers=num_workers,
+        pin_memory=torch.cuda.is_available(),
+        drop_last=True,
+        persistent_workers=True if num_workers > 0 else False,
+        collate_fn=collate_fn,
+    )
+    val_loader = DataLoader(
+        val_dataset,
+        batch_size=val_bs,
+        shuffle=False,
+        num_workers=num_workers,
+        pin_memory=torch.cuda.is_available(),
+        drop_last=False,
+        persistent_workers=True if num_workers > 0 else False,
+        collate_fn=collate_fn,
+    )
+    return train_loader, val_loader, vocab
+
+
+def collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, Any]:
+    """Custom collate function to assemble a batch of samples."""
+    images = torch.stack([item['image'] for item in batch])
+    # Extract nested labels.  Tag labels are stacked into a 2D tensor and
+    # rating labels are collected into a 1D tensor.
+    tag_labels = torch.stack([item['labels']['tags'] for item in batch])
+    rating_labels = torch.tensor([item['labels']['rating'] for item in batch], dtype=torch.long)
+    # Collate metadata lists and keep padding info for downstream usage.
+    metadata = {
+        'indices': [item['metadata']['index'] for item in batch],
+        'paths': [item['metadata']['path'] for item in batch],
+        'num_tags': torch.tensor([item['metadata']['num_tags'] for item in batch]),
+        'tags': [item['metadata']['tags'] for item in batch],
+        'ratings': [item['metadata']['rating'] for item in batch],
+        'scales': [item['metadata'].get('scale') for item in batch],
+        'pads': [item['metadata'].get('pad') for item in batch],
+    }
+    return {
+        'images': images,
+        'tag_labels': tag_labels,
+        'rating_labels': rating_labels,
+        'metadata': metadata,
+    }
\ No newline at end of file
diff --git a/src/loss_functions.py b/src/loss_functions.py
new file mode 100644
index 0000000..5ee6501
--- /dev/null
+++ b/src/loss_functions.py
@@ -0,0 +1,193 @@
+#!/usr/bin/env python3
+"""
+Loss functions for the anime image tagger.
+
+This module implements an asymmetric focal loss for multi‑label
+classification.  The pad class at index 0 is ignored when computing
+the tag loss.  This avoids penalising the model for predicting or
+missing the padding index, which is not a valid output class.
+"""
+
+from __future__ import annotations
+from typing import Dict, Optional, Tuple
+import logging
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+logger = logging.getLogger(__name__)
+
+
+class AsymmetricFocalLoss(nn.Module):
+    """
+    Asymmetric Focal Loss for multi‑label classification.
+
+    The loss ignores the pad class at index 0.  Logits and targets are
+    sliced to remove this column before computing the loss.  Reduction
+    semantics are unchanged: ``mean`` averages the per‑element losses
+    across all remaining classes and samples, ``sum`` sums them, and
+    ``none`` returns the unreduced tensor.
+    """
+
+    def __init__(
+        self,
+        gamma_pos: float = 1.0,
+        gamma_neg: float = 3.0,
+        alpha: float = 0.75,
+        clip: float = 0.05,
+        reduction: str = 'mean',
+        label_smoothing: float = 0.05,
+    ):
+        super().__init__()
+        self.gamma_pos = gamma_pos
+        self.gamma_neg = gamma_neg
+        self.alpha = alpha
+        self.clip = clip
+        self.reduction = reduction
+        self.label_smoothing = label_smoothing
+
+    def forward(
+        self,
+        logits: torch.Tensor,
+        targets: torch.Tensor,
+        sample_weights: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        """
+        Compute the asymmetric focal loss.
+
+        Args:
+            logits: (B, num_classes) raw logits.
+            targets: (B, num_classes) binary targets.
+            sample_weights: (B,) optional per‑sample weights.
+
+        Returns:
+            Loss value.
+        """
+        # Detach targets to prevent gradient flow
+        targets = targets.detach()
+        # Ignore pad class at index 0 by slicing
+        if logits.size(1) > 1:
+            logits = logits[:, 1:]
+            targets = targets[:, 1:]
+        # Apply label smoothing if specified
+        if self.label_smoothing > 0:
+            targets = targets * (1 - self.label_smoothing) + self.label_smoothing / 2
+        # Calculate probabilities
+        probs = torch.sigmoid(logits)
+        # Clip probabilities to prevent log(0)
+        probs = torch.clamp(probs, min=self.clip, max=1.0 - self.clip)
+        # Calculate focal weights
+        pos_weights = targets * torch.pow(1 - probs, self.gamma_pos)
+        neg_weights = (1 - targets) * torch.pow(probs, self.gamma_neg)
+        # Binary cross entropy
+        bce = -(targets * torch.log(probs) + (1 - targets) * torch.log(1 - probs))
+        # Apply focal weights with unified alpha
+        focal_loss = self.alpha * (pos_weights * bce + neg_weights * bce)
+        # Apply sample weights if provided (for frequency-based sampling)
+        if sample_weights is not None:
+            if sample_weights.dim() == 1:
+                sample_weights = sample_weights.unsqueeze(1)
+            focal_loss = focal_loss * sample_weights
+        # Reduction
+        if self.reduction == 'mean':
+            return focal_loss.mean()
+        elif self.reduction == 'sum':
+            return focal_loss.sum()
+        else:
+            return focal_loss
+
+
+class MultiTaskLoss(nn.Module):
+    """
+    Combined loss for tag prediction and rating classification.
+
+    A simple weighted combination of the asymmetric focal loss for tags
+    and the cross‑entropy loss for ratings.
+    """
+
+    def __init__(
+        self,
+        tag_loss_weight: float = 0.9,
+        rating_loss_weight: float = 0.1,
+        tag_loss_fn: Optional[nn.Module] = None,
+        rating_loss_fn: Optional[nn.Module] = None,
+    ):
+        super().__init__()
+        self.tag_loss_weight = tag_loss_weight
+        self.rating_loss_weight = rating_loss_weight
+        self.tag_loss_fn = tag_loss_fn or AsymmetricFocalLoss()
+        self.rating_loss_fn = rating_loss_fn or nn.CrossEntropyLoss(
+            label_smoothing=0.1
+        )
+
+    def forward(
+        self,
+        tag_logits: torch.Tensor,
+        rating_logits: torch.Tensor,
+        tag_targets: torch.Tensor,
+        rating_targets: torch.Tensor,
+        sample_weights: Optional[torch.Tensor] = None,
+    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
+        """
+        Compute combined loss for tags and ratings.
+
+        Args:
+            tag_logits: (B, num_tags) logits for tag prediction.
+            rating_logits: (B, num_ratings) logits for rating classification.
+            tag_targets: (B, num_tags) binary targets for tags.
+            rating_targets: (B,) or (B, num_ratings) targets for ratings.
+            sample_weights: Optional per‑sample weights.
+        """
+        tag_loss = self.tag_loss_fn(tag_logits, tag_targets, sample_weights)
+        # Compute rating loss
+        if rating_targets.dim() == 2:
+            rating_targets = rating_targets.argmax(dim=1)
+        rating_loss = self.rating_loss_fn(rating_logits, rating_targets)
+        total_loss = (
+            self.tag_loss_weight * tag_loss +
+            self.rating_loss_weight * rating_loss
+        )
+        losses = {
+            'total': total_loss,
+            'tag_loss': tag_loss,
+            'rating_loss': rating_loss,
+        }
+        return total_loss, losses
+
+
+class FrequencyWeightedSampler:
+    """
+    Helper class to compute frequency‑based sample weights.
+
+    Used during training to balance common vs rare tags.  Tags with high
+    frequency receive lower weights, while rare tags receive higher weights.
+    """
+
+    def __init__(
+        self,
+        tag_frequencies: torch.Tensor,
+        weighting_type: str = 'sqrt_inverse',
+        min_weight: float = 0.1,
+        max_weight: float = 10.0,
+    ):
+        self.weighting_type = weighting_type
+        self.min_weight = min_weight
+        self.max_weight = max_weight
+        # Precompute weights per class (excluding pad index)
+        self.weights = self._compute_weights(tag_frequencies)
+
+    def _compute_weights(self, freqs: torch.Tensor) -> torch.Tensor:
+        """Compute weights inversely proportional to frequencies."""
+        # Avoid division by zero
+        freqs = freqs.float().clamp(min=1)
+        if self.weighting_type == 'inverse':
+            weights = 1.0 / freqs
+        elif self.weighting_type == 'sqrt_inverse':
+            weights = 1.0 / torch.sqrt(freqs)
+        elif self.weighting_type == 'log_inverse':
+            weights = 1.0 / torch.log(freqs + 1.0)
+        else:
+            raise ValueError(f"Unknown weighting_type: {self.weighting_type}")
+        weights = weights / weights.max()
+        weights = torch.clamp(weights, self.min_weight, self.max_weight)
+        return weights
\ No newline at end of file
diff --git a/src/metrics.py b/src/metrics.py
new file mode 100644
index 0000000..acf65f2
--- /dev/null
+++ b/src/metrics.py
@@ -0,0 +1,108 @@
+#!/usr/bin/env python3
+"""
+Evaluation metrics for multi‑label classification.
+
+This module provides simple functions to compute precision, recall, F1 and
+mean average precision for multi‑label predictions.  The pad class at index 0
+is excluded from all computations by default.
+"""
+
+from __future__ import annotations
+from typing import Dict, Iterable, Tuple
+import torch
+
+
+def compute_precision_recall_f1(
+    probabilities: torch.Tensor,
+    targets: torch.Tensor,
+    threshold: float = 0.5,
+    pad_index: int = 0,
+) -> Dict[str, float]:
+    """
+    Compute micro‑averaged precision, recall and F1 score for multi‑label
+    predictions at a given threshold.
+
+    Args:
+        probabilities: Tensor of shape (B, C) with predicted probabilities.
+        targets: Tensor of shape (B, C) with binary ground‑truth labels.
+        threshold: Threshold above which a probability is considered a positive prediction.
+        pad_index: Index of the pad class to ignore.  Both probabilities and
+            targets will have this column removed before computation.
+
+    Returns:
+        Dictionary with keys ``precision``, ``recall`` and ``f1``.
+    """
+    if probabilities.dim() != 2 or targets.dim() != 2:
+        raise ValueError("probabilities and targets must be 2D tensors")
+    # Remove pad column
+    probs = probabilities.clone()
+    targs = targets.clone()
+    if pad_index is not None:
+        probs = torch.cat([probs[:, :pad_index], probs[:, pad_index+1:]], dim=1)
+        targs = torch.cat([targs[:, :pad_index], targs[:, pad_index+1:]], dim=1)
+    preds = (probs >= threshold).float()
+    true = targs.float()
+    tp = (preds * true).sum()
+    fp = ((preds == 1) & (true == 0)).float().sum()
+    fn = ((preds == 0) & (true == 1)).float().sum()
+    precision = tp / (tp + fp + 1e-8)
+    recall = tp / (tp + fn + 1e-8)
+    f1 = 2 * precision * recall / (precision + recall + 1e-8)
+    return {'precision': precision.item(), 'recall': recall.item(), 'f1': f1.item()}
+
+
+def average_precision_score(
+    probabilities: torch.Tensor,
+    targets: torch.Tensor,
+    pad_index: int = 0,
+) -> Dict[str, float]:
+    """
+    Compute mean average precision (mAP) across classes excluding the pad class.
+
+    A simple approximation of average precision is computed by sorting samples
+    by predicted probability for each class and summing the precision
+    increments at each true positive.  Classes with no positive examples are
+    skipped.
+
+    Args:
+        probabilities: Tensor of shape (B, C) with predicted probabilities.
+        targets: Tensor of shape (B, C) with binary ground‑truth labels.
+        pad_index: Index of the pad class to ignore.
+
+    Returns:
+        Dictionary with keys ``map`` (mean AP across classes) and
+        ``per_class_ap`` (list of APs for each non‑pad class).
+    """
+    if probabilities.dim() != 2 or targets.dim() != 2:
+        raise ValueError("probabilities and targets must be 2D tensors")
+    probs = probabilities.clone()
+    targs = targets.clone()
+    if pad_index is not None:
+        probs = torch.cat([probs[:, :pad_index], probs[:, pad_index+1:]], dim=1)
+        targs = torch.cat([targs[:, :pad_index], targs[:, pad_index+1:]], dim=1)
+    B, C = probs.shape
+    ap_values = []
+    for c in range(C):
+        scores = probs[:, c]
+        labels = targs[:, c]
+        # Skip classes with no positive examples
+        total_pos = labels.sum().item()
+        if total_pos == 0:
+            continue
+        # Sort by predicted score descending
+        sorted_indices = torch.argsort(scores, descending=True)
+        sorted_labels = labels[sorted_indices]
+        cum_tp = sorted_labels.cumsum(dim=0).float()
+        precisions = cum_tp / torch.arange(1, len(cum_tp) + 1, device=probs.device).float()
+        recalls = cum_tp / total_pos
+        # Integrate precision over recall using step function
+        ap = 0.0
+        prev_recall = 0.0
+        for p, r in zip(precisions, recalls):
+            dr = r.item() - prev_recall
+            if dr > 0:
+                ap += p.item() * dr
+                prev_recall = r.item()
+        ap_values.append(ap)
+    mean_ap = sum(ap_values) / len(ap_values) if ap_values else 0.0
+    return {'map': mean_ap, 'per_class_ap': ap_values}
\ No newline at end of file
diff --git a/src/utils/vocabulary.py b/src/utils/vocabulary.py
new file mode 100644
index 0000000..6decb29
--- /dev/null
+++ b/src/utils/vocabulary.py
@@ -0,0 +1,257 @@
+#!/usr/bin/env python3
+"""
+Shared vocabulary module for anime image tagger.
+
+This module provides a unified TagVocabulary class that handles tag
+management for both training and inference.  The pad token at index 0
+is reserved for masking purposes and **is not** a valid output class.
+Downstream loss functions and metrics must ignore this index when
+computing gradients or scores.
+"""
+
+from __future__ import annotations
+import json
+import logging
+from pathlib import Path
+from typing import Dict, Iterable, List, Optional
+import torch
+
+logger = logging.getLogger(__name__)
+
+
+class TagVocabulary:
+    """Unified tag vocabulary manager with support for both training and inference.
+
+    This class combines functionality from both data loading and inference
+    code to provide a single, comprehensive vocabulary implementation.
+    The special ``pad_token`` occupies index 0 and is used solely for
+    padding/masking; it should never be considered a valid label during
+    training or evaluation.  The ``unk_token`` at index 1 is used to
+    represent unknown or rare tags.
+    """
+
+    def __init__(self, vocab_path: Optional[Path] = None, min_frequency: int = 1) -> None:
+        """Initialize vocabulary, optionally loading from file.
+
+        Args:
+            vocab_path: Optional path to vocabulary file to load.
+            min_frequency: Minimum frequency for tags to be included when building vocabulary.
+        """
+        self.tag_to_index: Dict[str, int] = {}
+        self.index_to_tag: Dict[int, str] = {}
+        self.tag_frequencies: Dict[str, int] = {}
+        self.min_frequency = min_frequency
+
+        # Special tokens
+        self.pad_token = "<PAD>"
+        self.unk_token = "<UNK>"
+
+        # Rating classes (fixed)
+        self.rating_to_index: Dict[str, int] = {
+            "general": 0,
+            "sensitive": 1,
+            "questionable": 2,
+            "explicit": 3,
+            "unknown": 4,
+        }
+
+        # Convenience attributes for compatibility
+        self.tags: List[str] = []
+        # Unknown index will be updated when vocabulary is built/loaded
+        self.unk_index: int = 1
+
+        # If a vocabulary file is supplied, attempt to load it
+        if vocab_path is not None and vocab_path.exists():
+            try:
+                self.load_vocabulary(vocab_path)
+            except Exception:
+                logger.info(f"Could not load vocabulary from {vocab_path}, will build a new one")
+
+    def __len__(self) -> int:
+        """Return the size of the vocabulary."""
+        return len(self.tag_to_index)
+
+    def encode_tags(self, tags: Iterable[str]) -> torch.Tensor:
+        """Encode a list of tag strings into a multi‑hot tensor.
+
+        Unknown tags are mapped to the ``<UNK>`` index; the resulting tensor
+        has shape ``(vocab_size,)`` and dtype ``float32``.
+
+        Args:
+            tags: Iterable of tag strings.
+
+        Returns:
+            Multi‑hot tensor of shape ``(vocab_size,)``.
+        """
+        vector = torch.zeros(len(self.tag_to_index), dtype=torch.float32)
+        for tag in tags:
+            idx = self.tag_to_index.get(tag, self.tag_to_index.get(self.unk_token, self.unk_index))
+            vector[idx] = 1.0
+        return vector
+
+    def get_tag_index(self, tag: str) -> int:
+        """Get index for a tag (for inference compatibility).
+
+        Args:
+            tag: Tag string.
+
+        Returns:
+            Index of the tag, or ``unk_index`` if not found.
+        """
+        return self.tag_to_index.get(tag, self.unk_index)
+
+    def get_tag_from_index(self, index: int) -> str:
+        """Get tag from index (for inference compatibility).
+
+        Args:
+            index: Tag index.
+
+        Returns:
+            Tag string, or ``<UNK>`` if index not found.
+        """
+        return self.index_to_tag.get(index, self.unk_token)
+
+    def build_from_annotations(self, json_files: List[Path], top_k: Optional[int]) -> None:
+        """Build a vocabulary from a collection of JSON annotation files.
+
+        Args:
+            json_files: List of annotation files to parse.
+            top_k: Maximum number of tags to keep (sorted by frequency).  If ``None``,
+                all tags meeting ``min_frequency`` will be included.
+        """
+        logger.info(f"Building vocabulary from {len(json_files)} annotation files")
+        tag_counts: Dict[str, int] = {}
+        for json_file in json_files:
+            try:
+                with open(json_file, 'r', encoding='utf-8') as f:
+                    data = json.load(f)
+                for entry in data:
+                    tags_field = entry.get('tags')
+                    if not tags_field:
+                        continue
+                    tags_list: List[str]
+                    if isinstance(tags_field, str):
+                        tags_list = tags_field.split()
+                    elif isinstance(tags_field, list):
+                        tags_list = tags_field
+                    else:
+                        continue
+                    for tag in tags_list:
+                        tag_counts[tag] = tag_counts.get(tag, 0) + 1
+            except Exception as e:
+                logger.warning(f"Failed to parse {json_file}: {e}")
+        # Sort tags by frequency and cut to top_k
+        sorted_tags = sorted(
+            [t for t, c in tag_counts.items() if c >= self.min_frequency],
+            key=lambda x: (-tag_counts[x], x)
+        )
+        if top_k is not None and top_k > 0:
+            sorted_tags = sorted_tags[:top_k]
+        # Assign indices. Reserve 0 for <PAD> and 1 for <UNK>
+        self.tag_to_index = {self.pad_token: 0, self.unk_token: 1}
+        self.index_to_tag = {0: self.pad_token, 1: self.unk_token}
+        self.unk_index = 1
+        for idx, tag in enumerate(sorted_tags, start=2):
+            self.tag_to_index[tag] = idx
+            self.index_to_tag[idx] = tag
+            self.tag_frequencies[tag] = tag_counts[tag]
+        self.tags = sorted_tags
+        logger.info(f"Vocabulary built with {len(self.tag_to_index)} tags (including special tokens)")
+
+    def save_vocabulary(self, vocab_path: Path) -> None:
+        """Save the vocabulary to a JSON file.
+
+        The file contains ``tag_to_index``, ``index_to_tag`` and ``tag_frequencies``.
+
+        Args:
+            vocab_path: Path to save vocabulary file.
+        """
+        vocab_path = Path(vocab_path)
+        vocab_path.parent.mkdir(parents=True, exist_ok=True)
+        with open(vocab_path, 'w', encoding='utf-8') as f:
+            json.dump({
+                'tag_to_index': self.tag_to_index,
+                'index_to_tag': self.index_to_tag,
+                'tag_frequencies': self.tag_frequencies,
+            }, f, ensure_ascii=False, indent=2)
+        logger.info(f"Saved vocabulary to {vocab_path}")
+
+    def save(self, filepath: Path) -> None:
+        """Alias for :meth:`save_vocabulary` for compatibility."""
+        self.save_vocabulary(filepath)
+
+    def load_vocabulary(self, vocab_path: Path) -> None:
+        """Load vocabulary from a JSON file.
+
+        Args:
+            vocab_path: Path to vocabulary JSON file.
+        """
+        with open(vocab_path, 'r', encoding='utf-8') as f:
+            data = json.load(f)
+        self.tag_to_index = data['tag_to_index']
+        self.index_to_tag = {int(k): v for k, v in data['index_to_tag'].items()}
+        self.tag_frequencies = data.get('tag_frequencies', {})
+        # Ensure special tokens are present
+        for token in (self.pad_token, self.unk_token):
+            if token not in self.tag_to_index:
+                idx = len(self.tag_to_index)
+                self.tag_to_index[token] = idx
+                self.index_to_tag[idx] = token
+        # Update unk_index
+        self.unk_index = self.tag_to_index.get(self.unk_token, 1)
+        # Update tags list for compatibility (exclude special tokens)
+        self.tags = [tag for tag in self.tag_to_index.keys() if tag not in (self.pad_token, self.unk_token)]
+        logger.info(f"Loaded vocabulary with {len(self.tag_to_index)} tags from {vocab_path}")
+
+    @classmethod
+    def from_file(cls, filepath: Path) -> 'TagVocabulary':
+        """Load vocabulary from a simple text file (one tag per line).
+
+        This method is for backward compatibility with inference code.
+
+        Args:
+            filepath: Path to text file with tags.
+        """
+        with open(filepath, 'r', encoding='utf-8') as f:
+            tags = [line.strip() for line in f if line.strip()]
+        vocab = cls()
+        vocab.tag_to_index = {vocab.pad_token: 0, vocab.unk_token: 1}
+        vocab.index_to_tag = {0: vocab.pad_token, 1: vocab.unk_token}
+        vocab.unk_index = 1
+        for idx, tag in enumerate(tags, start=2):
+            vocab.tag_to_index[tag] = idx
+            vocab.index_to_tag[idx] = tag
+        vocab.tags = tags
+        return vocab
+
+def load_vocabulary_for_training(vocab_dir: Path) -> TagVocabulary:
+    """Load vocabulary from directory (for backward compatibility).
+
+    First tries to load from ``vocabulary.json``, then ``tags.txt``,
+    otherwise creates a dummy vocabulary.
+
+    Args:
+        vocab_dir: Directory containing vocabulary files.
+
+    Returns:
+        TagVocabulary instance.
+    """
+    vocab_json = vocab_dir / "vocabulary.json"
+    if vocab_json.exists():
+        vocab = TagVocabulary()
+        vocab.load_vocabulary(vocab_json)
+        return vocab
+    vocab_file = vocab_dir / "tags.txt"
+    if vocab_file.exists():
+        return TagVocabulary.from_file(vocab_file)
+    logger.warning(f"Vocabulary file not found in {vocab_dir}, using dummy vocabulary")
+    dummy_tags = [f"tag_{i}" for i in range(1000)]
+    vocab = TagVocabulary()
+    vocab.tag_to_index = {vocab.pad_token: 0, vocab.unk_token: 1}
+    vocab.index_to_tag = {0: vocab.pad_token, 1: vocab.unk_token}
+    vocab.unk_index = 1
+    for idx, tag in enumerate(dummy_tags, start=2):
+        vocab.tag_to_index[tag] = idx
+        vocab.index_to_tag[idx] = tag
+    vocab.tags = dummy_tags
+    return vocab
\ No newline at end of file
diff --git a/src/models/vit_patchify.py b/src/models/vit_patchify.py
new file mode 100644
index 0000000..d4d3e7f
--- /dev/null
+++ b/src/models/vit_patchify.py
@@ -0,0 +1,116 @@
+#!/usr/bin/env python3
+"""
+Patch embedding utilities for Vision Transformer models.
+
+This module provides simple functions to partition images into non‑overlapping
+patches and, optionally, generate padding masks.  When images have been
+letterbox‑resized to preserve aspect ratio, the padded regions should not
+contribute to the attention mechanism of a Vision Transformer.  The
+``compute_padding_mask`` function uses the padding information returned by
+``letterbox_resize`` to mark patches that fall entirely within padded areas.
+
+Functions
+---------
+    patchify(images: Tensor, patch_size: int) -> Tensor
+        Convert a batch of images into a batch of patch embeddings.
+
+    compute_padding_mask(pads: Iterable[Tuple[int, int, int, int]],
+                         out_sizes: Iterable[Tuple[int, int]],
+                         patch_size: int) -> torch.Tensor
+        Generate a boolean mask indicating which patches correspond to
+        padding for each image in a batch.
+
+Example
+-------
+>>> from src.data.HDF5_loader import letterbox_resize
+>>> image = torch.rand(3, 480, 640)
+>>> img_padded, info = letterbox_resize(image, 640, (114,114,114), 16)
+>>> patches = patchify(img_padded.unsqueeze(0), 16)
+>>> mask = compute_padding_mask([info['pad']], [info['out_size']], 16)
+>>> patches.shape, mask.shape
+((1, 1600, 768), (1, 1600))
+"""
+
+from __future__ import annotations
+from typing import Iterable, List, Tuple
+import torch
+
+def patchify(images: torch.Tensor, patch_size: int) -> torch.Tensor:
+    """Convert a batch of images into a batch of flattened patches.
+
+    Args:
+        images: Tensor of shape (B, C, H, W).
+        patch_size: Size of each square patch.  Both H and W must be
+            divisible by ``patch_size``.
+
+    Returns:
+        Tensor of shape (B, num_patches, C * patch_size * patch_size)
+        containing flattened patches.
+    """
+    if images.dim() != 4:
+        raise ValueError(f"images must be 4D (B,C,H,W), got shape {images.shape}")
+    B, C, H, W = images.shape
+    if H % patch_size != 0 or W % patch_size != 0:
+        raise ValueError(f"Image dimensions ({H},{W}) must be divisible by patch_size ({patch_size})")
+    # Number of patches along height and width
+    n_patches_h = H // patch_size
+    n_patches_w = W // patch_size
+    # Use unfold to extract sliding blocks without overlap
+    patches = images.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)
+    # patches shape: (B, C, n_patches_h, n_patches_w, patch_size, patch_size)
+    # Rearrange to (B, n_patches_h * n_patches_w, C, patch_size, patch_size)
+    patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()
+    patches = patches.view(B, n_patches_h * n_patches_w, C * patch_size * patch_size)
+    return patches
+
+
+def compute_padding_mask(
+    pads: Iterable[Tuple[int, int, int, int]],
+    out_sizes: Iterable[Tuple[int, int]],
+    patch_size: int,
+) -> torch.Tensor:
+    """Generate a padding mask for patchified images.
+
+    Given per‑image padding information (left, top, right, bottom) and the
+    output image sizes, this function computes a boolean mask of shape
+    (B, num_patches) where ``True`` indicates that a patch lies completely
+    within the padded region and should therefore be ignored by attention.
+
+    Args:
+        pads: Iterable of tuples ``(pad_left, pad_top, pad_right, pad_bottom)``.
+        out_sizes: Iterable of tuples ``(H_out, W_out)`` corresponding to the
+            sizes of the padded images.
+        patch_size: Size of each patch used during patchification.
+
+    Returns:
+        Boolean tensor of shape (B, num_patches) where ``True`` marks padded
+        patches.
+    """
+    pad_list = list(pads)
+    size_list = list(out_sizes)
+    if len(pad_list) != len(size_list):
+        raise ValueError("pads and out_sizes must have the same length")
+    batch = len(pad_list)
+    masks: List[torch.Tensor] = []
+    for i in range(batch):
+        pad_left, pad_top, pad_right, pad_bottom = pad_list[i]
+        H_out, W_out = size_list[i]
+        n_h = H_out // patch_size
+        n_w = W_out // patch_size
+        mask = torch.zeros(n_h, n_w, dtype=torch.bool)
+        # Compute valid region boundaries (in patch coordinates)
+        # left boundary: first patch whose left edge is >= pad_left
+        valid_start_x = pad_left // patch_size
+        # top boundary: first patch whose top edge is >= pad_top
+        valid_start_y = pad_top // patch_size
+        # right boundary: last valid patch index (exclusive)
+        valid_end_x = (W_out - pad_right) // patch_size
+        valid_end_y = (H_out - pad_bottom) // patch_size
+        # Fill mask with True (padded) then set valid region to False
+        mask[:, :] = True
+        mask[valid_start_y:valid_end_y, valid_start_x:valid_end_x] = False
+        masks.append(mask.reshape(-1))
+    return torch.stack(masks, dim=0)
+
+
+__all__ = ["patchify", "compute_padding_mask"]
\ No newline at end of file
diff --git a/src/models/__init__.py b/src/models/__init__.py
new file mode 100644
index 0000000..138012b
--- /dev/null
+++ b/src/models/__init__.py
@@ -0,0 +1,10 @@
+"""Model utilities for the anime tagger.
+
+This package exposes helper functions for patchification and optional
+padding masks used by Vision Transformer models.  Additional model
+definitions can be added here in the future.
+"""
+
+from .vit_patchify import patchify, compute_padding_mask  # noqa: F401
+
+__all__ = ["patchify", "compute_padding_mask"]
\ No newline at end of file
diff --git a/configs/train_config.yaml b/configs/train_config.yaml
new file mode 100644
index 0000000..0550806
--- /dev/null
+++ b/configs/train_config.yaml
@@ -0,0 +1,28 @@
+# Training data configuration
+
+# This file contains the data‑specific parameters for the anime tagger
+# training pipeline.  These values override defaults in
+# ``SimplifiedDataConfig``.  In particular, random cropping is disabled by
+# setting ``random_crop_scale`` to (1.0, 1.0), and ``pad_color`` is set
+# to a neutral grey (114, 114, 114) to match the letterbox padding used
+# during inference.  ``image_size`` must be divisible by ``patch_size`` to
+# ensure that no pixels are lost when partitioning the image into
+# non‑overlapping patches.
+
+data:
+  # Final square size to which images are resized before being split
+  # into patches.  Must be divisible by ``patch_size``.
+  image_size: 640
+  # Size of each patch used by the vision transformer.  If
+  # ``image_size`` is not a multiple of ``patch_size`` then additional
+  # padding will be applied automatically.
+  patch_size: 16
+  # Disable random cropping so that images are not randomly cropped when
+  # scaling down.  A scale of (1.0, 1.0) means the entire letterboxed
+  # image is used.
+  random_crop_scale: [1.0, 1.0]
+  # Colour used for letterbox padding (RGB values in 0‑255 range).  A
+  # neutral grey of 114 is commonly used for YOLO models.
+  pad_color: [114, 114, 114]
+  # Note: ``image_size`` must be divisible by ``patch_size`` to prevent
+  # dropping edge pixels when partitioning into patches.
\ No newline at end of file
