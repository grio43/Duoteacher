--- a/Dataset_Analysis.py
+++ b/Dataset_Analysis.py
@@ -21,15 +21,39 @@
 
 import numpy as np
 from PIL import Image
-import cv2
+
 import matplotlib.pyplot as plt
-import seaborn as sns
-from tqdm import tqdm
-import networkx as nx
-from sklearn.decomposition import PCA
-from sklearn.manifold import TSNE
-from sklearn.cluster import DBSCAN
-import h5py
+
+
+# Optional heavy dependencies with feature flags
+try:
+    import cv2
+    CV2_AVAILABLE = True
+except ImportError:
+    CV2_AVAILABLE = False
+
+try:
+    import seaborn as sns
+    SEABORN_AVAILABLE = True
+except ImportError:
+    SEABORN_AVAILABLE = False
+
+try:
+    from tqdm import tqdm as _tqdm
+    TQDM_AVAILABLE = True
+    def tqdm(iterable=None, **kwargs):
+        return _tqdm(iterable, **kwargs)
+except ImportError:
+    TQDM_AVAILABLE = False
+    def tqdm(iterable=None, **kwargs):
+        # Fallback: no-op progress wrapper
+        return iterable
+
+try:
+    import networkx as nx
+    NETWORKX_AVAILABLE = True
+except ImportError:
+    NETWORKX_AVAILABLE = False
 
 # Optional imports with fallback
 try:
@@ -45,6 +69,15 @@
 except ImportError:
     IMAGEHASH_AVAILABLE = False
     warnings.warn("imagehash not available. Install with: pip install imagehash")
+
+# Optional wordcloud
+try:
+    from wordcloud import WordCloud
+    WORDCLOUD_AVAILABLE = True
+except ImportError:
+    WORDCLOUD_AVAILABLE = False
+    warnings.warn("wordcloud not available. Install with: pip install wordcloud")
+
 
 # Configure logging
 logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
@@ -230,16 +263,20 @@
                         stats.mean_color = tuple(float(x) for x in img_array.mean(axis=(0, 1)))
                         stats.std_color = tuple(float(x) for x in img_array.std(axis=(0, 1)))
                         
-                        # Brightness (average of grayscale)
-                        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
-                        stats.brightness = float(gray.mean())
-                        
-                        # Contrast (standard deviation of grayscale)
-                        stats.contrast = float(gray.std())
-                        
-                        # Sharpness (variance of Laplacian)
-                        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
-                        stats.sharpness = float(laplacian.var())
+                        # Brightness/contrast (grayscale)
+                        if CV2_AVAILABLE:
+                            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
+                            stats.brightness = float(gray.mean())
+                            stats.contrast = float(gray.std())
+                            # Sharpness (variance of Laplacian)
+                            laplacian = cv2.Laplacian(gray, cv2.CV_64F)
+                            stats.sharpness = float(laplacian.var())
+                        else:
+                            # Fallback without OpenCV
+                            gray = (0.2989 * img_array[...,0] + 0.5870 * img_array[...,1] + 0.1140 * img_array[...,2]).astype(float)
+                            stats.brightness = float(gray.mean())
+                            stats.contrast = float(gray.std())
+                            # sharpness left as None without cv2
                     except Exception as e:
                         # Color stats are optional, log but don't fail
                         logger.debug(f"Could not extract color stats for {image_path}: {e}")
@@ -445,7 +482,12 @@
                     similarity = cooc / denominator if denominator > 0 else 0
                     similarity_matrix[i, j] = similarity
         
-        # Cluster using DBSCAN
+        # Cluster using DBSCAN (if scikit-learn available)
+        try:
+            from sklearn.cluster import DBSCAN  # deferred import
+        except Exception:
+            logger.warning("scikit-learn not available, skipping clustering")
+            return []
         try:
             clustering = DBSCAN(eps=1-min_similarity, min_samples=2, metric='precomputed')
             distance_matrix = 1 - similarity_matrix
@@ -562,6 +604,13 @@
         
         if not image_paths:
             logger.warning("No images found to analyze")
+            self.dataset_stats.analysis_duration_seconds = (datetime.now() - start_time).total_seconds()
+            # Optionally generate an empty report for consistency
+            if self.config.generate_report:
+                try:
+                    self._generate_report()
+                except Exception as e:
+                    logger.error(f"Error generating empty report: {e}")
             return self.dataset_stats
         
         # Analyze images
@@ -753,9 +802,16 @@
         logger.info("Creating visualizations...")
         
         try:
-            # Set style
-            plt.style.use('seaborn-v0_8-darkgrid')
-            sns.set_palette("husl")
+            # Set style (fall back if seaborn style missing)
+            try:
+                plt.style.use('seaborn-v0_8-darkgrid')
+            except Exception:
+                try:
+                    plt.style.use('seaborn-darkgrid')
+                except Exception:
+                    plt.style.use('default')
+            if SEABORN_AVAILABLE:
+                sns.set_palette("husl")
             
             # Create figure with subplots
             fig = plt.figure(figsize=(20, 12))
@@ -849,6 +905,8 @@
     
     def _create_tag_wordcloud(self):
         """Create a word cloud of tags"""
+        if not WORDCLOUD_AVAILABLE:
+            return
         fig = None
         try:
             # Prepare tag frequencies
