diff --git a/Configuration_System.py b/Configuration_System.py
--- a/Configuration_System.py
+++ b/Configuration_System.py
@@ -38,7 +38,7 @@
     """Types of configuration files"""
     TRAINING = "training"
     INFERENCE = "inference"
-    VALIDATION = "validation"
+    MODEL = "model"
     EXPORT = "export"
     PREPROCESSING = "preprocessing"
     FULL = "full"
@@ -68,6 +68,8 @@
                 value = value.to_dict(exclude_private)
             elif isinstance(value, list):
                 value = [v.to_dict(exclude_private) if is_dataclass(v) else v for v in value]
+            elif isinstance(value, tuple):
+                value = list(value)
             elif isinstance(value, dict):
                 value = {k: v.to_dict(exclude_private) if is_dataclass(v) else v 
                         for k, v in value.items()}
@@ -231,7 +233,7 @@
     
     # Vision specific
     image_size: int = 640
-    patch_size: int = 14
+    patch_size: int = 16
     num_channels: int = 3
     
     # Special tokens
@@ -368,38 +370,60 @@
     def validate(self):
         """Validate data configuration"""
         errors = []
-        
-        # Validate storage locations
+
+        # Validate storage locations and check unique priorities
+        priorities = []
         for i, loc in enumerate(self.storage_locations):
             try:
                 storage_loc = StorageLocation(**loc)
                 storage_loc.validate()
+                priorities.append(storage_loc.priority)
             except Exception as e:
                 errors.append(f"Storage location {i}: {str(e)}")
-        
+        # Unique priority validation
+        if len(priorities) != len(set(priorities)):
+            dupes = [p for p in set(priorities) if priorities.count(p) > 1]
+            errors.append(f"Duplicate storage location priorities detected: {sorted(dupes)}")
+
         if self.batch_size <= 0:
             errors.append(f"batch_size must be positive, got {self.batch_size}")
-        
+
         if self.num_workers < 0:
             errors.append(f"num_workers must be non-negative, got {self.num_workers}")
-        
+
         if self.cache_size_gb < 0:
             errors.append(f"cache_size_gb must be non-negative, got {self.cache_size_gb}")
-        
+
+        # New bounds checks
+        if self.prefetch_factor < 1:
+            errors.append(f"prefetch_factor must be >= 1, got {self.prefetch_factor}")
+        if self.preload_files < 0:
+            errors.append(f"preload_files must be >= 0, got {self.preload_files}")
+        if self.random_rotation_degrees < 0:
+            errors.append(f"random_rotation_degrees must be >= 0, got {self.random_rotation_degrees}")
+
         # Validate normalization parameters
         for param_name, param_value in [('normalize_mean', self.normalize_mean), 
                                         ('normalize_std', self.normalize_std)]:
             if len(param_value) != 3:
                 errors.append(f"{param_name} must have 3 values, got {len(param_value)}")
-        
+
+        # Validate pad_color
+        if len(self.pad_color) != 3:
+            errors.append(f"pad_color must have 3 values, got {len(self.pad_color)}")
+        else:
+            for i, c in enumerate(self.pad_color):
+                if not isinstance(c, int) or not (0 <= c <= 255):
+                    errors.append(f"pad_color[{i}] must be int in [0,255], got {c}")
+
         # Validate augmentation parameters
         if self.random_flip_prob < 0 or self.random_flip_prob > 1:
             errors.append(f"random_flip_prob must be in [0, 1], got {self.random_flip_prob}")
-        
+
         scale_min, scale_max = self.random_crop_scale
         if not (0 < scale_min <= scale_max <= 1):
             errors.append(f"random_crop_scale must satisfy 0 < min <= max <= 1, got {self.random_crop_scale}")
-        
+
         if errors:
             raise ConfigValidationError("Data config validation failed:\n" + "\n".join(errors))
 
@@ -481,6 +505,15 @@
         """Validate training configuration"""
         errors = []
         
+        # cuDNN benchmark can conflict with deterministic execution
+        if self.deterministic and self.benchmark:
+            logger.warning("deterministic=True forces benchmark=False to ensure repeatability")
+            self.benchmark = False
+        
+        # AMP backend note
+        if self.use_amp and self.amp_opt_level:
+            logger.warning("amp_opt_level appears to target NVIDIA Apex; if using torch.cuda.amp, prefer configuring amp_dtype and ignore amp_opt_level")
+        
         if self.learning_rate <= 0:
             errors.append(f"learning_rate must be positive, got {self.learning_rate}")
         
@@ -574,31 +607,52 @@
     def validate(self):
         """Validate inference configuration"""
         errors = []
-        
+
+        # Security posture warnings if API is enabled
+        if self.enable_api:
+            if self.api_host == '0.0.0.0':
+                logger.warning("API is bound to 0.0.0.0; consider 127.0.0.1 or a firewall in production")
+            if self.api_cors_origins and ('*' in self.api_cors_origins):
+                logger.warning("API CORS origins allow '*'; require explicit origins for production")
+
         if self.model_path and not Path(self.model_path).exists():
             logger.warning(f"Model path does not exist: {self.model_path}")
-        
+
         if not 0 <= self.prediction_threshold <= 1:
             errors.append(f"prediction_threshold must be in [0, 1], got {self.prediction_threshold}")
-        
+
         if self.min_predictions > self.max_predictions:
             errors.append(
                 f"min_predictions ({self.min_predictions}) > max_predictions ({self.max_predictions})"
             )
-        
+
         if self.min_predictions < 0:
             errors.append(f"min_predictions must be non-negative, got {self.min_predictions}")
-        
+
         valid_formats = ["json", "text", "csv", "xml", "yaml"]
         if self.output_format not in valid_formats:
             errors.append(f"Unknown output_format: {self.output_format}. Must be one of {valid_formats}")
-        
+
         if self.score_decimal_places < 0 or self.score_decimal_places > 10:
             errors.append(f"score_decimal_places must be in [0, 10], got {self.score_decimal_places}")
-        
+
         if self.api_port < 1 or self.api_port > 65535:
             errors.append(f"api_port must be in [1, 65535], got {self.api_port}")
-        
+
+        # Additional field validations
+        if self.top_k is not None and self.top_k <= 0:
+            errors.append(f"top_k must be > 0 when set, got {self.top_k}")
+        if self.api_workers < 1:
+            errors.append(f"api_workers must be >= 1, got {self.api_workers}")
+        if self.api_rate_limit < 0:
+            errors.append(f"api_rate_limit must be >= 0, got {self.api_rate_limit}")
+        if self.cache_ttl_seconds < 0:
+            errors.append(f"cache_ttl_seconds must be >= 0, got {self.cache_ttl_seconds}")
+        if self.max_batch_size <= 0:
+            errors.append(f"max_batch_size must be >= 1, got {self.max_batch_size}")
+        if self.batch_timeout_ms < 0:
+            errors.append(f"batch_timeout_ms must be >= 0, got {self.batch_timeout_ms}")
+
         if errors:
             raise ConfigValidationError("Inference config validation failed:\n" + "\n".join(errors))
 
@@ -739,11 +793,6 @@
         self.config_type = config_type
         self.config = self._create_default_config()
         self.config_history: List[Dict[str, Any]] = []
-        self._env_pattern = re.compile(
-            r'^([A-Z0-9_]+)__([A-Z0-9_]+)(?:__(.+))?$',
-            re.IGNORECASE
-        )
-    
     def _create_default_config(self) -> BaseConfig:
         """Create default configuration based on type"""
         config_map = {
@@ -796,40 +845,46 @@
     def save_to_file(self, path: Union[str, Path], backup: bool = True):
         """
         Save configuration to file
-        
+
         Args:
             path: Output path
             backup: Whether to create backup if file exists
         """
         path = Path(path)
-        
+
+        # Normalize extension first so backups follow the final filename
+        if path.suffix not in ['.yaml', '.yml', '.json']:
+            path = path.with_suffix('.yaml')
+
         # Create backup if requested and file exists
         if backup and path.exists():
             backup_path = path.with_suffix(f'.backup_{datetime.now():%Y%m%d_%H%M%S}{path.suffix}')
             path.rename(backup_path)
             logger.info(f"Created backup at {backup_path}")
-        
+
         # Create directory if needed
         path.parent.mkdir(parents=True, exist_ok=True)
-        
+
         # Save based on extension
         if path.suffix in ['.yaml', '.yml']:
             self.config.to_yaml(path)
         elif path.suffix == '.json':
             self.config.to_json(path)
         else:
-            # Default to YAML
-            path = path.with_suffix('.yaml')
-            self.config.to_yaml(path)
-        
+            # Fallback to YAML
+            self.config.to_yaml(path.with_suffix('.yaml'))
+
         logger.info(f"Saved config to {path}")
-    
+
+
     def update_from_env(self, prefix: str = "ANIME_TAGGER_"):
         """
         Update configuration from environment variables
         
-        Format: ANIME_TAGGER_<SECTION>__<FIELD>[__<NESTED_FIELD>]
-        Example: ANIME_TAGGER_TRAINING__LEARNING_RATE=0.001
+        Format: ANIME_TAGGER_<SECTION>__<FIELD>[__<SUBFIELD>[__<...>]]
+        Examples:
+            ANIME_TAGGER_TRAINING__LEARNING_RATE=0.001
+            ANIME_TAGGER_MODEL__HEADS__ATTN=16
         """
         updates = defaultdict(dict)
         
@@ -848,20 +903,20 @@
             # Parse value
             parsed_value = self._parse_env_value(value)
             
-            # Build nested update dictionary
-            if len(parts) == 2:
-                section, field = parts
-                updates[section][field] = parsed_value
-            elif len(parts) == 3:
-                section, field, subfield = parts
-                if section not in updates:
-                    updates[section] = {}
-                if field not in updates[section]:
-                    updates[section][field] = {}
-                updates[section][field][subfield] = parsed_value
-            else:
-                logger.warning(f"Too many nesting levels in env var: {key}")
-        
+            # Build nested update dictionary for arbitrary depth
+            section, *rest = parts
+            if section not in updates:
+                updates[section] = {}
+            cursor = updates[section]
+            if not rest:
+                logger.warning(f"Missing field name in env var: {key}")
+                continue
+            for subkey in rest[:-1]:
+                if subkey not in cursor or not isinstance(cursor[subkey], dict):
+                    cursor[subkey] = {}
+                cursor = cursor[subkey]
+            cursor[rest[-1]] = parsed_value
+
         # Apply updates
         if updates:
             self._apply_nested_updates(self.config, dict(updates))
@@ -899,7 +954,7 @@
         updates = {}
         
         for key, value in vars(args).items():
-            if value is not None and key != 'config' and key != 'output_config':
+            if value is not None and key not in {'config', 'output_config', 'validate_only'}:
                 updates[key] = value
         
         if updates:
@@ -1031,7 +1086,7 @@
     # General arguments
     parser.add_argument('--config', type=str, help='Path to config file')
     parser.add_argument('--output-config', type=str, help='Save final config to file')
-    parser.add_argument('--validate-only', action='store_true', help='Only validate config and exit')
+    parser.add_argument('--validate-only', action='store_true', default=None, help='Only validate config and exit')
     
     # Create subparsers for different config sections
     if config_type == ConfigType.FULL:
@@ -1055,7 +1110,7 @@
         train_group.add_argument('--training.learning_rate', type=float, help='Learning rate')
         train_group.add_argument('--training.weight_decay', type=float, help='Weight decay')
         train_group.add_argument('--training.device', type=str, help='Device (cuda/cpu)')
-        train_group.add_argument('--training.distributed', action='store_true', help='Use distributed training')
+        train_group.add_argument('--training.distributed', action='store_true', default=None, help='Use distributed training')
         train_group.add_argument('--training.seed', type=int, help='Random seed')
         
         # Inference arguments
@@ -1063,13 +1118,13 @@
         infer_group.add_argument('--inference.model_path', type=str, help='Path to model')
         infer_group.add_argument('--inference.prediction_threshold', type=float, help='Prediction threshold')
         infer_group.add_argument('--inference.top_k', type=int, help='Top-k predictions')
-        infer_group.add_argument('--inference.filter_nsfw', action='store_true', help='Filter NSFW tags')
+        infer_group.add_argument('--inference.filter_nsfw', action='store_true', default=None, help='Filter NSFW tags')
         
         # Export arguments
         export_group = parser.add_argument_group('export')
         export_group.add_argument('--export.export_format', type=str, help='Export format')
-        export_group.add_argument('--export.optimize', action='store_true', help='Optimize exported model')
-        export_group.add_argument('--export.quantize', action='store_true', help='Quantize model')
+        export_group.add_argument('--export.optimize', action='store_true', default=None, help='Optimize exported model')
+        export_group.add_argument('--export.quantize', action='store_true', default=None, help='Quantize model')
     
     return parser
 
@@ -1238,6 +1293,7 @@
         os.environ["ANIME_TAGGER_TRAINING__LEARNING_RATE"] = "0.0005"
         os.environ["ANIME_TAGGER_MODEL__NUM_GROUPS"] = "25"
         os.environ["ANIME_TAGGER_DATA__BATCH_SIZE"] = "64"
+        os.environ["ANIME_TAGGER_MODEL__NUM_LABELS"] = "250000"
         
         manager.update_from_env()
         
